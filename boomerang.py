# boomerang.py (fixed: normalize early-hours assigned to previous day)
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import xml.etree.ElementTree as ET
import xml.sax.saxutils as sax

# ========== CONFIG ==========
CHANNEL_ID = "boomerang"
CHANNEL_NAME = "CARTOONITO"
BASE_URL = "https://info.msky.vn/vn/Boomerang.html?date={date}"  # date in dd/mm/YYYY
OUTPUT_FILE = "boomerang.xml"
VN_TZ = timezone(timedelta(hours=7))
# ============================


def fetch_html_for_date(date_str):
    """T·∫£i HTML c·ªßa trang cho date_str ƒë·ªãnh d·∫°ng dd/mm/YYYY"""
    url = BASE_URL.format(date=date_str)
    print(f"Fetching: {url}")
    resp = requests.get(url, timeout=20)
    resp.raise_for_status()
    resp.encoding = "utf-8"
    return resp.text


def parse_table_rows(html, base_date):
    """
    Parse b·∫£ng EPG -> tr·∫£ v·ªÅ list item d·∫°ng dict:
      { 'start_dt': datetime(tz=VN_TZ), 'title_vi': str, 'title_en': str_or_empty }
    S·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c last_dt ƒë·ªÉ x·ª≠ l√Ω rollover qua ng√†y ti·∫øp theo.
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table")
    if not table:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b·∫£ng EPG")
        return []

    rows = table.find_all("tr")
    items = []

    last_dt = None

    # b·ªè header n·∫øu c√≥
    for tr in rows[1:]:
        tds = tr.find_all("td")
        if not tds or len(tds) < 2:
            continue

        time_str = tds[0].get_text(strip=True)
        title_vi = tds[1].get_text(strip=True) if len(tds) > 1 else ""
        title_en = tds[2].get_text(strip=True) if len(tds) > 2 else ""

        # chu·∫©n h√≥a time_str
        if not time_str or ":" not in time_str:
            # b·ªè n·∫øu kh√¥ng c√≥ gi·ªù
            continue

        # ƒë·∫£m b·∫£o gi·ªù d·∫°ng HH:MM
        parts = time_str.split(":")
        hh = parts[0].zfill(2)
        mm = parts[1].zfill(2)
        time_norm = f"{hh}:{mm}"

        # t·∫°o datetime t·∫°m d·ª±a tr√™n base_date
        try:
            start_dt = datetime.strptime(f"{base_date.strftime('%d/%m/%Y')} {time_norm}", "%d/%m/%Y %H:%M")
            start_dt = start_dt.replace(tzinfo=VN_TZ)
        except Exception as e:
            print(f"‚ö†Ô∏è B·ªè qua d√≤ng th·ªùi gian kh√¥ng parse ƒë∆∞·ª£c: '{time_str}' ({e})")
            continue

        # N·∫øu ƒë√£ c√≥ last_dt v√† start_dt <= last_dt => ch∆∞∆°ng tr√¨nh ƒë√£ sang ng√†y ti·∫øp theo
        if last_dt is not None:
            while start_dt <= last_dt:
                start_dt += timedelta(days=1)

        # append item (stop t√≠nh sau)
        items.append({
            "start_dt": start_dt,
            "title_vi": title_vi,
            "title_en": title_en,
            "duration_min": None  # c√≥ th·ªÉ g√°n n·∫øu c√≥ c·ªôt th·ªùi l∆∞·ª£ng
        })

        last_dt = start_dt

    return items


def compute_stops(items):
    """
    T·ª´ danh s√°ch items (c√≥ start_dt), t√≠nh stop_dt:
    - n·∫øu c√≥ duration_min th√¨ d√πng
    - ng∆∞·ª£c l·∫°i stop = start c·ªßa ch∆∞∆°ng tr√¨nh ti·∫øp theo
    - ch∆∞∆°ng tr√¨nh cu·ªëi m·∫∑c ƒë·ªãnh 30 ph√∫t
    """
    for i, it in enumerate(items):
        start = it["start_dt"]
        dur = it.get("duration_min")
        if dur and isinstance(dur, int) and dur > 0:
            stop = start + timedelta(minutes=dur)
        else:
            if i + 1 < len(items):
                stop = items[i + 1]["start_dt"]
                # n·∫øu stop <= start (kh√¥ng h·ª£p l·ªá) th√¨ c·ªông 1 ng√†y ƒë·ªÉ an to√†n
                if stop <= start:
                    stop += timedelta(days=1)
            else:
                stop = start + timedelta(minutes=30)
        it["stop_dt"] = stop
    return items


def normalize_early_hours_to_base_date(items, base_date):
    """
    M·ªôt s·ªë site li·ªát k√™ 00:00-03:xx ·ªü ph·∫ßn ƒë·∫ßu c·ªßa b·∫£ng v√† ch√∫ng c√≥ th·ªÉ b·ªã g√°n l√πi v·ªÅ ng√†y tr∆∞·ªõc.
    N·∫øu m·ª•c c√≥ start_dt.date() < base_date v√† gi·ªù < 4 (00..03), ta +1 ng√†y ƒë·ªÉ ƒë∆∞a v·ªÅ base_date.
    ƒêi·ªÅu n√†y gi·ªØ c√°c ch∆∞∆°ng tr√¨nh 00:00..03:59 thu·ªôc ng√†y hi·ªán t·∫°i.
    """
    adjusted = 0
    for it in items:
        sd = it["start_dt"]
        if sd.date() < base_date and sd.hour < 4:
            # shift forward until date == base_date (tr√°nh shift qu√°)
            while sd.date() < base_date:
                sd = sd + timedelta(days=1)
            # adjust stop_dt t∆∞∆°ng ·ª©ng (n·∫øu stop_dt <= old start => c·ªông c√πng s·ªë ng√†y)
            delta_days = (sd.date() - it["start_dt"].date()).days
            it["start_dt"] = sd
            if "stop_dt" in it and it["stop_dt"] is not None:
                it["stop_dt"] = it["stop_dt"] + timedelta(days=delta_days)
            adjusted += 1
    if adjusted:
        print(f"üîß ƒê√£ ƒëi·ªÅu ch·ªânh {adjusted} m·ª•c early-hours sang ng√†y {base_date.isoformat()}")
    return items


def filter_only_today(items, base_date):
    """
    L·ªçc ch·ªâ gi·ªØ ch∆∞∆°ng tr√¨nh c√≥ start thu·ªôc ng√†y base_date (gi·ªù VN).
    base_date: datetime.date (VN)
    """
    start_day = datetime.combine(base_date, datetime.min.time()).replace(tzinfo=VN_TZ)
    end_day = start_day + timedelta(days=1)
    filtered = [it for it in items if start_day <= it["start_dt"] < end_day]
    return filtered


def build_xml(items, output_file=OUTPUT_FILE):
    tv = ET.Element("tv", {
        "source-info-name": "msky.vn",
        "generator-info-name": "lps_crawler"
    })

    ch = ET.SubElement(tv, "channel", {"id": CHANNEL_ID})
    ET.SubElement(ch, "display-name").text = CHANNEL_NAME
    ET.SubElement(ch, "url").text = "https://info.msky.vn/vn/Boomerang.html"

    for it in items:
        start_s = it["start_dt"].strftime("%Y%m%d%H%M%S ") + "+0700"
        stop_s = it["stop_dt"].strftime("%Y%m%d%H%M%S ") + "+0700"
        prog = ET.SubElement(tv, "programme", {
            "start": start_s,
            "stop": stop_s,
            "channel": CHANNEL_ID
        })
        ET.SubElement(prog, "title", {"lang": "vi"}).text = sax.escape(it["title_vi"] or "")
        if it.get("title_en"):
            ET.SubElement(prog, "title", {"lang": "en"}).text = sax.escape(it["title_en"])

    tree = ET.ElementTree(tv)
    try:
        ET.indent(tree, space="  ", level=0)
    except Exception:
        pass
    tree.write(output_file, encoding="utf-8", xml_declaration=True)
    print(f"‚úÖ Xu·∫•t th√†nh c√¥ng {output_file} ({len(items)} programmes)")


def main():
    now_vn = datetime.now(VN_TZ)
    base_date = now_vn.date()
    date_str = base_date.strftime("%d/%m/%Y")

    html = fetch_html_for_date(date_str)
    items = parse_table_rows(html, base_date)
    if not items:
        # fallback: n·∫øu kh√¥ng parse ƒë∆∞·ª£c, v·∫´n t·∫°o file r·ªóng / header
        print("‚ö†Ô∏è Kh√¥ng c√≥ item n√†o t·ª´ parse_table_rows()")
        build_xml([], OUTPUT_FILE)
        return

    items = compute_stops(items)

    # --- CH·ªñ S·ª¨A: normalize early hours (00:00-03:59) n·∫øu b·ªã g√°n v·ªÅ ng√†y tr∆∞·ªõc
    items = normalize_early_hours_to_base_date(items, base_date)

    # L·ªçc ch·ªâ gi·ªØ ch∆∞∆°ng tr√¨nh start thu·ªôc ng√†y hi·ªán t·∫°i (VN)
    filtered = filter_only_today(items, base_date)

    # Debug prints (in danh s√°ch start ƒë·ªÉ anh ki·ªÉm tra)
    print("=== Program starts (all parsed) ===")
    for it in items[:200]:
        print(it["start_dt"].strftime("%Y-%m-%d %H:%M:%S %z"), "-", it["title_vi"])

    print("=== Program starts (filtered = today) ===")
    for it in filtered[:200]:
        print(it["start_dt"].strftime("%Y-%m-%d %H:%M:%S %z"), "-", it["title_vi"])

    build_xml(filtered, OUTPUT_FILE)


if __name__ == "__main__":
    print("=== RUNNING CRAWLER ===")
    try:
        main()
    except Exception as e:
        print("‚ö†Ô∏è L·ªói:", e)
    print("=== DONE ===")
