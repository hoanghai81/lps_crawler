name: daily-epg-crawl

on:
  schedule:
    - cron: '0 18 * * *' # 01:00 Asia/Ho_Chi_Minh (UTC+7)
  workflow_dispatch: {}

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # core packages required by craw.py
          pip install requests beautifulsoup4 python-dateutil pytz lxml pyyaml
          # if repository provides requirements.txt, install additional deps
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run crawler
        run: |
          python3 craw.py

      - name: Verify epg produced
        run: |
          if [ ! -f epg.xml ]; then echo "epg.xml not found" && exit 2; fi
          echo "epg.xml generated:" && wc -c epg.xml || true

      # Option A: Commit epg.xml back to repo (uncomment if you want automatic commit)
      - name: Commit epg.xml
        if: always()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add epg.xml || true
          git commit -m "Auto update epg.xml: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" || echo "No changes to commit"
          git push || echo "Push failed"

      # Option B: Upload as workflow artifact (keeps build outputs without modifying repo)
      - name: Upload epg artifact
        uses: actions/upload-artifact@v4
        with:
          name: epg-xml
          path: epg.xml
          
